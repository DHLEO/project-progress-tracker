<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Progress Tracker</title>
    <link rel="stylesheet" href="styles/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>A Dynamic Credit Risk Prediction System Based on Federated Learning</h1>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="#overview" class="active">Overview</a></li>
                <li><a href="#team">Team</a></li>
                <li><a href="#updates">Updates</a></li>
                <li><a href="#interim-report">Interim Report</a></li>
                <li><a href="#milestone">Milestone</a></li>
                <li><a href="#deliverables">Deliverables</a></li>
            </ul>
        </div>
    </nav>

    <main>
        <div class="container">
            <section id="overview">
                <h2>Project Overview</h2>
                <div class="card">         
                    <div class="project-stats">
                        <div class="stat">
                            <h3>Start Date</h3>
                            <p>Jan 20, 2025</p>
                        </div>
                        <div class="stat">
                            <h3>End Date</h3>
                            <p>July 31, 2025</p>
                        </div>
                        <div class="stat">
                            <h3>Status</h3>
                            <p class="status in-progress">In Progress</p>
                        </div>
                        <div class="stat">
                            <h3>Completion</h3>
                            <div class="progress-bar">
                                <div class="progress" style="width: 85%;">85%</div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section id="team">
                <h2>Team Members</h2>
                <div class="team-grid">
                    <div class="team-member">
                        <div class="member-photo"></div>
                        <h3>Huang RuiFan</h3>
                        <p class="role">Leader</p>
                        <p class="email">u3638104@connect.hku.hk</p>
                    </div>
                    <div class="team-member">
                        <div class="member-photo"></div>
                        <h3>He Fuzhi</h3>
                        <p class="role">Member</p>
                        <p class="email">u3638289@connect.hku.hk</p>
                    </div>
                    <div class="team-member">
                        <div class="member-photo"></div>
                        <h3>He Yanze</h3>
                        <p class="role">Member</p>
                        <p class="email">u3627468@connect.hku.hk</p>
                    </div>
                    <div class="team-member">
                        <div class="member-photo"></div>
                        <h3>Liu Dahai</h3>
                        <p class="role">Member</p>
                        <p class="email">u3638304@connect.hku.hk</p>
                    </div>
                </div>
            </section>

            <section id="updates">
                <h2>Progress Updates</h2>
                <div class="updates-list">
                    <div class="update-item milestone" data-markdown="<h1>Project Progress Update 4</h1>

<h2>Achievements</h2>

<p>We conducted a four-class classification task on the MIT credit rating dataset using both independent training and horizontal federated learning. The data was evenly split across three clients under an IID setting, where each client received the same number of samples and maintained the global class distribution. All clients used the same neural network architecture. Independent training was performed for 30 local epochs per client, while federated learning involved 15 rounds of aggregation with 6 local epochs per round (i.e., 90 total local epochs).</p>

<p>Each client was assigned 17,112 samples, with 13,689 for training and 3,423 for testing. The class distribution was identical across all clients, roughly reflecting the global distribution: around 62.7% for class P2, 11.3% for P1, 14.5% for P3, and 11.5% for P4. This balanced IID setup ensured that learning performance was not affected by data heterogeneity and enabled fair comparison between training strategies.</p>

<p>In the IID setting, independently trained models achieved high validation accuracy, with Client 1 reaching 95.97% at epoch 24 and Client 2 reaching 95.59% at epoch 26. This demonstrates strong convergence and minimal overfitting. The federated learning model, expected to aggregate diverse client updates, is anticipated to perform comparably or even better due to consistent data distribution and increased generalization. Federated learning also offers the benefit of collaborative learning without sharing raw data, making it suitable for privacy-sensitive applications.</p>

![Four-Class Classification Results on MIT Credit Rating Dataset](images/4.png)

<h2>Challenges</h2>

<p>Even under IID data settings, subtle differences or inconsistencies in local training processes across clients can still impact the global model's performance. These differences might arise from variations in client hardware, local optimization strategies, or even slight discrepancies in data processing techniques. As a result, despite the data being independently and identically distributed, the local models may not converge at the same rate or even in the same direction, leading to suboptimal global model performance. Therefore, ensuring greater consistency in client updates, such as harmonizing training protocols, model architectures, and data preprocessing steps, is crucial to mitigate the risk of performance degradation. Addressing this issue involves refining federated learning frameworks to ensure that local models can consistently contribute to the global model, improving its robustness across diverse client environments.</p>

<p>Federated learning is often focused on creating a global model that can generalize well across all clients. However, this goal becomes increasingly challenging when clients possess distinct data distributions or have unique specific needs. While a global model trained on the collective data of all clients might perform well on average, it may fail to capture the nuances of individual client data, thus hindering its ability to meet personalized requirements. Balancing the generalization of the global model with the need for personalization is an ongoing challenge. This requires advanced techniques such as multi-task learning or fine-tuning, where each client can refine the global model to better suit their specific data distributions. Moreover, leveraging adaptive aggregation strategies that prioritize more relevant or critical clients can help tailor the global model while maintaining its overall effectiveness. Balancing both the global and local objectives in federated learning remains a central research challenge that requires innovative solutions to achieve optimal performance across diverse real-world applications.</p>

<h2>Plans</h2>

<p>To address the challenges of client data heterogeneity, the plan focuses on improving data preprocessing and standardization across all clients. This ensures that the models trained on different data sources are more aligned. By using techniques such as normalization or standardization, we can reduce the impact of diverse data distributions. Additionally, we plan to enhance the stability of local model training by using adaptive optimization techniques like Adam or RMSprop. To further address these issues, aggregation strategies will be adjusted, possibly introducing weighted aggregation or considering client-specific performance to improve the global model.</p>

<p>For personalization, we aim to incorporate multi-task learning to allow each client to have a more personalized model while still benefiting from global training. This approach ensures that both the global model's performance and the clients' specific needs are balanced. Additionally, after federated training, fine-tuning will be applied locally to further refine the model for each client's data distribution. By utilizing a model ensemble, we can combine individual client models or adjust their weights to better optimize personalized predictions, ultimately improving the overall performance of the federated system.</p>">
                        <div class="update-date">July 7, 2025</div>
                        <div class="update-content">
                            <h3>Progress Update 4</h3>
                            <p>Comprehensive four-class classification experiments on MIT credit rating dataset comparing independent training vs horizontal federated learning under IID settings. Achieved high validation accuracy and identified key challenges in client consistency and personalization.</p>
                            <div class="view-details"><i class="fas fa-file-alt"></i> View Details</div>
                        </div>
                    </div>
                    <div class="update-item milestone" data-markdown="<h1>Project Progress Update 3</h1>

<h2>1. Achievements</h2>

<h3>1.1 Privacy Protection Technology</h3>

<h4>1.1.1 Flower Framework</h4>
<p>Flower is an open-source federated learning framework dedicated to supporting the training and deployment of distributed machine learning models. It achieves collaborative model optimization without centralized data (e.g., via the FedAvg algorithm) through a client-server architecture, is compatible with mainstream frameworks such as PyTorch and TensorFlow, features a modular design and privacy-preserving characteristics (e.g., reducing data transmission and supporting differential privacy), and provides a flexible and efficient solution for distributed machine learning in privacy-sensitive scenarios such as edge devices, healthcare, and finance.</p>

<p>Flower itself does not directly provide built-in privacy-preserving algorithms, but it supports various privacy protection mechanisms through architectural design and extensible interfaces.</p>

<p><strong>Fundamental Privacy-Preserving Mechanisms:</strong></p>
<p>The core principle is data localization, where raw data is not shared: the local data of participants (clients) remains on-site at all times, with only model parameters (e.g., gradients, weights) uploaded to the server, thus avoiding direct exposure of raw data. Subsequent decentralized training is performed entirely on client-side devices, with the server only aggregating the global model. In terms of secure parameter transmission, Flower natively supports encrypted transmission via HTTPS or gRPC to prevent man-in-the-middle attacks from eavesdropping on gradients or model parameters.</p>

<p><strong>Advanced Privacy-Preserving Techniques (Extensible):</strong></p>
<p>Flower's modular design enables users to implement advanced privacy-preserving techniques through extensions:</p>
<ul>
<li><strong>Differential Privacy (DP):</strong> Noise (e.g., Gaussian noise) is added to gradients before clients upload them, ensuring that individual data cannot be inferred from the gradients.</li>
<li><strong>Secure Aggregation:</strong> Techniques such as multi-party encrypted aggregation use cryptographic methods (e.g., MPC) to prevent the server from observing contributions from individual clients during gradient aggregation.</li>
<li><strong>Trusted Execution Environment (TEE):</strong> The aggregation logic on the server side is executed within a hardware-secured area (e.g., Intel SGX's Enclave), ensuring that even server administrators cannot access the data in memory.</li>
<li><strong>Gradient Clipping:</strong> Limits the range of gradient values (e.g., L2 norm) to reduce the amount of raw data information embedded in gradients.</li>
</ul>

<h4>1.1.2 SecretFlow</h4>
<p>SecretFlow natively integrates multiple encrypted computation virtual devices, including MPC, TEE, TECC, and homomorphic encryption, to enable flexible selection, and provides a rich set of federated learning algorithms and differential privacy mechanisms. It aims to reduce the technical barriers for developers and users of privacy computing through a well-designed layered architecture and out-of-the-box functionalities for privacy-preserving data analysis and machine learning.</p>

<p>In the device layer of the SecretFlow framework, the fundamental unit is the SecretFlow Processing Unit (SPU). The SPU provides a unified programmable device abstraction, where privacy computing technologies (e.g., MPC) are abstracted as encrypted devices, and single-party local computation is abstracted as plaintext devices. The SPU backend, which is based on MPC, essentially operates as a distributed system responsible for handling communication and coordination in distributed environments.</p>

<p>Programming with SPU not only interfaces with AI frameworks (e.g., TensorFlow, JAX, PyTorch) to reduce the learning and development costs for AI developers but also provides pure secure semantic interfaces. By implementing only a small number of security protocols (e.g., addition, multiplication, AND, OR), complex models can be executed, allowing developers to focus more on security itself.</p>

<h3>1.2 Horizontal Federated Learning</h3>
<p>We conducted a federated learning experiment using 10,000 loan data records, distributed evenly across three clients. Each client retained the original bad loan distribution, and data did not overlap. A centralized server coordinated the training process using the FedAvg strategy, with all model evaluations conducted on a unified test set.</p>

<p>The system was built using the Flower framework and PyTorch. Each client locally trained a neural network and shared model parameters with the server. The server aggregated these updates to form a global model. After five training rounds, we compared the accuracy of the federated model with that of independently trained local models.</p>

<p>Results showed that federated learning preserved data privacy while improving model performance through distributed training. Accuracy improvements were recorded and visualized, demonstrating the effectiveness of knowledge aggregation across clients without direct data sharing.</p>

![Individual Training vs Federated Learning Accuracy Comparison](images/horizontal_fl_accuracy_update3.png)

<h2>2. Challenges</h2>

<p>In horizontal federated learning, the federated learning's performance exhibits limited improvement, potentially due to several interconnected factors:</p>

<p><strong>High Data Homogeneity:</strong> When client datasets share highly similar feature distributions (e.g., overlapping user behaviors or task types), locally trained models already approach optimal performance. Federated aggregation thus provides minimal additional gain, as seen in scenarios where individual client accuracies exceed 98% with negligible variance.</p>

<p><strong>Defective Aggregation Strategies:</strong> Standard algorithms like FedAvg use simplistic averaging, which fails to address non-IID data or client contribution disparities. This can cause dimensional collapse, where aggregated models lose feature diversity and struggle to generalize.</p>

<p><strong>Insufficient Training:</strong> Inadequate communication rounds or local epochs prevent models from converging. Synchronization delays (e.g., waiting for stragglers) further slow knowledge integration, especially in heterogeneous computing environments.</p>

<p><strong>Data Quality and Feature Analysis:</strong> The experimental baseline shows that the model achieved exceptionally high performance on the classification task for this dataset. We consider such high performance occurring simultaneously on both the client and federated learning sides to be an anomalous phenomenon.</p>

<p>In this round of experiments, we changed the algorithm model used in horizontal federated learning from a Random Forest to a Deep Neural Network (DNN). After switching from Random Forest to DNN, the model's performance exhibited an abnormal improvement. The possible reasons include:</p>

<p>The compatibility between the data characteristics and DNN may be key. Deep neural networks excel at capturing complex nonlinear relationships and high-order interactive features. Although the data was standardized, extreme values and potential multimodal distributions still exist. DNNs may better model these complex patterns through multi-layer nonlinear transformations, whereas the tree-based structure of Random Forests has limited fine-grained partitioning capability for such continuous numerical features.</p>

![Feature Correlation with Loan Status Analysis](images/feature_correlation_update3.png)

<p>Through analysis, the <strong>recoveries</strong> feature exhibits a correlation gain that far surpasses other features. This feature, which resembles &quot;bad debt recovery amount&quot; (i.e., the amount recovered through collection efforts after loan default), may indicate high-risk users when its value is elevated. Similarly, another highly correlated feature, <strong>debt_settle</strong> (likely referring to &quot;Debt Settlement&quot;), denotes whether debt restructuring has occurred.</p>

<p><strong>Vertical Federated Learning Challenges:</strong> In vertical federated learning, a complete model (such as a neural network) is artificially &quot;vertically&quot; split into different clients. This split may destroy the original structural integrity of the model. The contribution of features held by different clients to the final model prediction can vary greatly, leading to training bias towards clients with strong features and challenges in fair value quantification and benefit distribution.</p>

<p>For privacy, each client must compress its features into a low-dimensional embedding. If a client's local model is too simple (e.g., a shallow network), it acts as a &quot;lossy compression,&quot; failing to capture the full complexity of its local data, potentially losing critical predictive information before it reaches the server.</p>

<h2>3. Future Plans</h2>

<p>For horizontal federated learning, to alleviate the limitations mentioned above, federated systems should:</p>

<ul>
<li><strong>Promote Greater Data Heterogeneity:</strong> Strategically sample or augment client data and group clients with complementary feature distributions so that cross-client aggregation genuinely adds new information.</li>
<li><strong>Implement Robust Aggregation Schemes:</strong> Replace naïve FedAvg with more robust methods such as FedProx, FedNova, or attention-based and contribution-weighted methods to discourage dimensional collapse and reward informative updates.</li>
<li><strong>Extend Training Budgets:</strong> Use more communication rounds, adaptive local-epoch schedules, and asynchronous or straggler-tolerant protocols (e.g., FedBuff or partial-update acceptance) to ensure sufficient convergence despite heterogeneous computing resources.</li>
</ul>

<p>Together, these measures will substantially boost the generalization and stability of horizontally federated models, bringing us closer to a production-ready system for privacy-preserving credit risk prediction.</p>">
                        <div class="update-date">Jun 16, 2025</div>
                        <div class="update-content">
                            <h3>Progress Update 3</h3>
                            <p>Latest comprehensive update documenting advanced algorithm implementations, system architecture refinements, security enhancements, and detailed performance evaluations of our federated learning system.</p>
                            <div class="view-details"><i class="fas fa-file-alt"></i> View Details</div>
                        </div>
                    </div>
                    <div class="update-item milestone" data-markdown="<h1>Project Progress Update 2</h1>

<h2>1. Achievements</h2>

<h3>1.1 Data Preprocessing</h3>
<p>In the horizontal federated learning demo, we use the German credit dataset, which contains 
roughly 1,000 records described by 20 input features and a single binary target called kredit. 
In the raw data, about 70% of applicants are labeled as &quot;good&quot; credit (1) and 30% as &quot;bad&quot; 
credit (0). All categorical fields are represented by integer codes, and numeric fields span a 
range of values such as loan amount and duration.</p>

<p>Prior to modeling, categorical features are 
label-encoded and all numeric features are standardized to zero mean and unit variance. The 
full dataset is then split 80/20 into training (≈800 samples) and testing (≈200 samples) sets. 
The training set is further divided equally among three clients (≈266 samples each).</p>

<p>In the vertical federated learning demo, multiple data sources need to possess the following 
core characteristics. Firstly, there should be a high degree of overlap in sample IDs. A large 
number of identical sample identifiers must exist among different data sources, which serves 
as the basis for data alignment and joint modeling. By associating the data from various 
parties through the same sample IDs, collaborative learning can be achieved. Secondly, the 
features should be complementary. The features of each data source describe the samples 
from different dimensions, providing differentiated information. In this way, the data from 
multiple parties can be integrated to enhance the accuracy and generalization ability of the 
model.</p>

<p>Accordingly, in order to verify the feasibility of the algorithm in this round of experiment, we 
adopted the data collected by MIT, which evaluates the credit ratings of users from multiple 
aspects and with a high breadth of features. We divided the data equally into three parts and 
aligned the sample IDs. One of the parts, which contains the personal information of users, 
classifies the users' credit ratings into four categories from high to low. The other two parts of 
the data contain other bank and account features but do not include classification labels. 
Finally, after processing the missing values, dealing with the outliers, labelizing the 
character-type data, and standardizing the data, these three parts of data were put into use in 
this round of experiment.</p>

<h3>1.2 Horizontal Federated Learning Demo</h3>
<p>In this demo, we perform horizontal federated learning on the German Credit dataset, where 
each client holds a different subset of borrower records but uses the same feature set. We 
divide the data randomly into several shards and assign each shard to a Flower NumPyClient. 
Each client trains a small multilayer perceptron locally—three hidden layers of 64, 32, and 16 
units with ReLU activations and a sigmoid output—using a fixed number of mini‑batch 
gradient‑descent epochs (with optional L₂ regularization and gradient clipping). Clients never 
share raw data; they send only their model's parameter updates to the central server.</p>

<p>The Flower server collects these updates and computes a weighted average based on each 
client's number of samples to produce a new global model. To evaluate the benefit of 
collaboration, we also run a separate script that trains the same network architecture in two 
additional modes: entirely centralized (all data pooled) and isolated local (each shard 
individually). We then plot training and validation accuracy over communication rounds for 
all three scenarios. The results show that federated learning converges faster and achieves 
substantially higher validation accuracy than any single‑shard model, while approaching the 
performance of the fully centralized baseline—demonstrating that model quality can be 
greatly improved through collaborative training without ever exchanging private data.</p>

![Federated Learning vs. Centralized vs. Local Training Performance Comparison](images/nn_accuracy_comparison.png)

<h3>1.3 Vertical Federated Learning Demo</h3>
<p>After preprocessing, the data from MIT was divided into three sub-datasets. The data sample 
sizes in different sub-datasets are the same, but the features they contain are different. 
Therefore, the vertical federated learning architecture can be employed. The Secretflow 
architecture has a built-in basic framework for implementing vertical federated learning, 
which enables users to conveniently set up the server side and client sides according to their 
needs. Thus, the three datasets can be allocated to three parties, and one party can be selected 
as the server side. The FedNdArray data structure in Secretflow can directly receive the three 
datasets and automatically distribute them among the three parties.</p>

<p>In terms of model 
construction, Secretflow is highly compatible with the Pytorch architecture, allowing Pytorch 
models to be encapsulated for model training. After the data and model are prepared, a 
vertical federated learning demo can be successfully established. In this experiment, a simple 
MLP model was used, and the experimental results showed that the model trained through 
federated learning using the data from the three parties outperformed the model trained using 
the data from a single party. This outcome is consistent with expectations, as the data from 
the three parties expanded the data features, which is conducive to the learning of the model.</p>

<h2>2. Challenges Faced</h2>

<h3>2.1 Dataset</h3>
<p>We have thus far partitioned our original dataset into three disjoint subsets—either by 
allocating samples across the folds to simulate horizontal federated learning or by dividing 
feature sets among the parties to emulate a vertical federated scenario. We would greatly 
appreciate your guidance on whether there exist more suitable benchmark datasets—perhaps 
those characterized by pronounced statistical heterogeneity or richer feature modalities—or 
alternative approach that you would recommend for strengthening the rigor and realism of 
our federated learning experiments.</p>

<h3>2.2 Innovation</h3>
<p>We would welcome your insight as to whether our federated learning–based credit risk 
scoring project would benefit from one or more substantive methodological innovations to 
elevate its academic and practical impact. If you believe novel contributions are warranted, 
could you recommend specific directions?</p>

<h2>3. Future Plans</h2>

<p>Test on richer, multi‑modal credit datasets—such as those combining tabular borrower 
profiles, transaction sequences, and alternative data sources—while scaling our federation to 
dozens of simulated participants (with hierarchical aggregation to reduce communication 
overhead). Replace simple logistic‑regression baselines with advanced deep architectures (for 
example, transformers to model time‑series transaction behavior and graph neural networks 
to capture borrower–lender networks), and maybe introduce novel methodological 
enhancement to push the state of the art in our federated‑learning framework.</p>">
                        <div class="update-date">May 5, 2025</div>
                        <div class="update-content">
                            <h3>Progress Update 2</h3>
                            <p>Implemented horizontal and vertical federated learning demos using German credit dataset and MIT credit data, with detailed preprocessing and model training workflows. Identified challenges in dataset selection and opportunities for methodological innovation.</p>
                            <div class="view-details"><i class="fas fa-file-alt"></i> View Details</div>
                        </div>
                    </div>
                    <div class="update-item milestone" data-markdown="<h1>Achievements</h1>

<h2>Data Resources</h2>

Four datasets have been found that roughly meet the requirements and need to be used with discretion, not all of them may be usable.
(1) German Credit: Credit data from Germany. The original dataset contains 1000 entries with 20 categorical/symbolic attributes.In this dataset, each entry represents a person who received credit from a bank.Depending on the set of attributes, each person is categorized as either a good credit risk or a bad credit risk.
<br>
(2) Loan data: data on lending transactions from LendingClub (the world's largest peer-to-peer lending platform). Contains 1,800,000 consumer loans originated between 2014 and 2018.
<br>
(3) MIT credit ranking: data collected and designed by MIT to categorize credit rating scores.
P1: very good creditworthiness
P2: Good creditworthiness
P3: Fair credit
P4: Bad credit
These data are collected from financial institutions and contain a variety of characteristics related to customer demographics, credit history, and financial status.
<br>
(4) Real default: defaults that occurred in real scenarios (in fact this is the most granular data but feels the most difficult to process)

<h2>Federal Learning Framework</h2>

In this stage, we have conducted research on Python toolkit frameworks commonly used in federated learning, such as FedLab, Flower, and SecretFlow. All three frameworks are user-friendly, assisting users in building the client side and the server side, and implementing built-in or customized aggregation and comparison methods. We have implemented the official demos of SecretFlow, including horizontal federated learning (HFL) and vertical federated learning (VFL). Whether it is HFL or VFL, the data requires users to perform heterogeneous processing separately. Therefore, the selection, concatenation, and cleaning of the data are of great importance. In SecretFlow, the data will be distributed to different client sides, and the data content is protected through encryption methods to ensure data security. 

<h2>Official Documents:</h2>

(1) SecretFlow: https://www.secretflow.org.cn/en/docs/secretflow/main/tutorial
<br>
(2) Flower: https://flower.ai/docs/framework/main/en/index.html
<br>
(3) FedLab: https://fedlab.readthedocs.io/zh-cn/latest/index.html
<br>

<br>
<br>
<br>

<h1>Challenges</h1>

<h2>Data Challenges</h2>
(1) Data heterogeneity
There are often differences in data characteristics and distribution between datasets from different sources, i.e., data heterogeneity.This data heterogeneity can seriously affect the performance of a federated learning model, making it difficult for the model to achieve good results on data from all participants.
<br>
(2) Communication overhead
Federated learning requires transmission of model parameters among multiple participants.When the dataset size is huge and there are many participants, the frequent transmission of model parameters will generate huge communication overhead, which will also lead to a longer training process.Loan data and Real default two data files will have this problem.
<br>
(3) Inconsistent data formats
Different sources of datasets may use different data formats.Including different features of the different scale between the different features as well as the labeling of the way is not uniform.Failure to convert and standardize the format correctly can result in the data not being loaded smoothly into the federated learning system.
<br>
(4) Failure to ensure data quality

The data quality of different data sources may vary significantly, such as missing data, noisy data, etc.These low-quality data will affect the accuracy of model training and reduce model performance.

<h2>Algorithm Challenges</h2>
Challenges include dealing with semantic ambiguity (e.g., &quot;income&quot; vs. &quot;annual income&quot;), dynamic schema updates, and privacy constraints (e.g., hashing sensitive data).Challenges include resolving ID collisions between organizations, optimizing memory for large datasets, and managing label distribution bias.

<br>
<br>
<br>

<h1>Plans for Upcoming Weeks</h1>

<h2>Data Pre-processing</h2>
(1) Format unification: develop data format conversion tools to unify data sets in different formats into the standard format supported by the federal learning system to ensure that the data can be loaded smoothly.
<br>
(2) Quality enhancement: adopt data cleaning algorithms to remove noisy data and deal with missing values.For example, for numerical data, mean and median can be used to fill in the missing values; for text data, word embedding technology can be used for denoising and patching.
<br>
(3) annotation calibration: establish a unified annotation standard and calibrate annotation data from different sources by manual review and cross-validation to improve the consistency and accuracy of annotation.

<h2>Federated Learning Demo Implementation</h2>
Based on the data resources and federated learning framework research, we plan to implement a federated learning (FL) demo to validate the data preprocessing pipeline, system architecture, and algorithm deployment strategies. We will choose one of the three federated learning frameworks we have evaluated to build the demo environment.
<br> 
<br> 
For this demo, we will use four datasets: German Credit, Loan Data, MIT Credit Ranking, and Real Default. These datasets come from different sources, each with unique characteristics, which can reflect the data heterogeneity typically found in financial settings. Based on the specific properties of each dataset, we will implement either horizontal or vertical federated learning tasks to simulate a multi-party collaborative modeling process.
<br>
<br> 
During the demo, we will perform data cleaning, format unification, feature alignment, and label standardization, then distribute the data to different simulated client nodes for training. We will also incorporate performance evaluation metrics such as Accuracy, AUC, and F1-Score to assess the effectiveness of the model. The goal of this demo is to fully validate the entire federated learning workflow, from data preparation to model training, identify potential technical bottlenecks, and provide a practical foundation for building and optimizing large-scale federated learning systems in the future.
">
                        <div class="update-date">Apr 7, 2025</div>
                        <div class="update-content">
                            <h3>Progress Update 1</h3>
                            <p>Completed dataset selection and federated learning framework evaluation. Identified key challenges and developed strategic plans for data preprocessing and model implementation.</p>
                            <div class="view-details"><i class="fas fa-file-alt"></i> View Details</div>
                        </div>
                    </div>
                </div>
            </section>

            <section id="interim-report">
                <h2>Interim Report</h2>
                <div class="card">         
                    <div class="report-description">
                        <p>Our team has completed a comprehensive interim report documenting the current progress of our Dynamic Credit Risk Prediction System Based on Federated Learning project. This report provides detailed insights into our achievements, methodologies, and future research directions.</p>
                        
                        <h4>Report Highlights:</h4>
                        <ul>
                            <li><strong>Federated Learning Implementation:</strong> Detailed documentation of both horizontal and vertical federated learning demos</li>
                            <li><strong>Data Processing Pipeline:</strong> Comprehensive data preprocessing workflows and quality enhancement strategies</li>
                            <li><strong>System Architecture:</strong> Technical framework evaluation and implementation details</li>
                            <li><strong>Challenges & Solutions:</strong> Analysis of current challenges and proposed methodological innovations</li>
                            <li><strong>Future Roadmap:</strong> Strategic plans for remaining project phases and research directions</li>
                        </ul>
                    </div>
                    
                    <div class="report-actions">
                        <a href="Capstone Project Interim Report.pdf" target="_blank" class="download-btn">
                            <i class="fas fa-download"></i> Download Full Report
                        </a>
                    </div>
                </div>
            </section>

            <section id="milestone">
                <h2>Project Milestone</h2>
                <div class="timeline timeline-collapsed">
                    <div class="timeline-item visible">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">Jan 20 - Mar 24, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 1: Data Schema Mapping & Standardization</h3>
                            <p>Standardized cross-institutional data field mapping files (CSV/JSON) - 15 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item visible">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">Mar 25 - Apr 7, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 2: Horizontal FL Sample Partitioning Tool</h3>
                            <p>Python toolkit for dataset splitting by sample IDs in horizontal FL - 12 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item visible">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">Apr 8 - Apr 20, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 3: Vertical FL Feature Alignment</h3>
                            <p>PySpark-based encrypted feature matching module - 20 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item hidden">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">Apr 21 - May 5, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 4: OPRF-Based PSI Protocol Implementation</h3>
                            <p>Privacy-preserving ID alignment using OPRF - 25 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item hidden">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">May 6 - May 20, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 5: Differential Privacy Noise Injection Module</h3>
                            <p>Implementation of Laplace/Gaussian noise injection for data privacy - 18 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item hidden">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">May 21 - Jun 1, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 6: FedAvg/FedMA/FedProx Algorithm Integration</h3>
                            <p>Implementation and integration of core FL algorithms - 40 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item hidden">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">Jun 2 - Jun 10, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 7: Transformer Model Distillation Framework</h3>
                            <p>Development of model compression toolkit for knowledge transfer - 30 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item hidden">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">Jun 11 - Jun 20, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 8: CAE Label Obfuscation & CAFE Gradient Protection</h3>
                            <p>Implementation of security measures for model protection - 35 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item hidden">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">Jun 21 - Jun 30, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 9: Federated Cross-Validation System Development</h3>
                            <p>Development of validation system with AUC/KS tracking - 25 learning hours</p>
                        </div>
                    </div>
                    <div class="timeline-item hidden">
                        <div class="timeline-dot"></div>
                        <div class="timeline-date">Jul 1 - Jul 7, 2025</div>
                        <div class="timeline-content">
                            <h3>Task 10: Finalize and Evaluation</h3>
                            <p>Comprehensive analysis of model performance and security audits - 20 learning hours</p>
                        </div>
                    </div>
                </div>
                <div class="expand-section">
                    <button class="expand-btn" data-target="timeline">
                        <span class="expand-text">Show More <i class="fas fa-chevron-down"></i></span>
                        <span class="collapse-text">Show Less <i class="fas fa-chevron-up"></i></span>
                    </button>
                </div>
            </section>

            <section id="deliverables">
                <h2>Project Deliverables</h2>
                <div class="deliverables-list deliverables-collapsed">
                    <div class="deliverable-item visible">
                        <h3>Data Mapping Schemas</h3>
                        <p>Standardized cross-institutional data field mapping files (CSV/JSON) for consistent data representation across participating institutions.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: Mar 24, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item visible">
                        <h3>Horizontal Partitioning Toolkit</h3>
                        <p>Python toolkit for dataset splitting by sample IDs in horizontal federated learning scenarios.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: Apr 7, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item visible">
                        <h3>Vertical Alignment Tool</h3>
                        <p>PySpark-based encrypted feature matching module for vertical federated learning implementation.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: Apr 20, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item hidden">
                        <h3>PSI Protocol Library</h3>
                        <p>Python code for privacy-preserving ID alignment using Oblivious Pseudo-Random Function (OPRF).</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: May 5, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item hidden">
                        <h3>Noise Injection Module</h3>
                        <p>Python module injecting Laplace/Gaussian noise for enhanced data privacy protection.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: May 20, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item hidden">
                        <h3>FL Algorithm Suite</h3>
                        <p>Integrated PyTorch implementations of federated learning algorithms including FedAvg, FedMA, and FedProx.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: Jun 1, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item hidden">
                        <h3>Distillation Framework</h3>
                        <p>Transformer-based model compression toolkit for efficient knowledge transfer between models.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: Jun 10, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item hidden">
                        <h3>Security Toolkit</h3>
                        <p>CAE label obfuscator and CAFE gradient protection system for enhanced model security.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: Jun 20, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item hidden">
                        <h3>Validation System</h3>
                        <p>Federated cross-validation tool with AUC/KS tracking capabilities for model performance assessment.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: Jun 30, 2025</span>
                        </div>
                    </div>
                    <div class="deliverable-item hidden">
                        <h3>Final Evaluation</h3>
                        <p>Comprehensive analysis of model performance, security audits, and compliance certification documentation.</p>
                        <div class="deliverable-meta">
                            <span class="deliverable-date">Due: Jul 7, 2025</span>
                        </div>
                    </div>
                </div>
                <div class="expand-section">
                    <button class="expand-btn" data-target="deliverables">
                        <span class="expand-text">Show More <i class="fas fa-chevron-down"></i></span>
                        <span class="collapse-text">Show Less <i class="fas fa-chevron-up"></i></span>
                    </button>
                </div>
            </section>
        </div>
    </main>

    <div id="update-modal" class="modal">
        <div class="modal-content">
            <span class="close-modal">&times;</span>
            <div id="markdown-content" class="markdown-container"></div>
        </div>
    </div>

    <footer>
        <div class="container">
            <p>Last updated: <span id="last-updated">June 16, 2025</span></p>
        </div>
    </footer>

    <script src="js/main.js"></script>
</body>
</html> 
